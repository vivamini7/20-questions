{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9431804,"sourceType":"datasetVersion","datasetId":5730280},{"sourceId":85979,"sourceType":"modelInstanceVersion","modelInstanceId":72240,"modelId":76277}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 필요한 패키지 설치\n!pip install -q -U immutabledict sentencepiece\n\n# Gemma PyTorch 라이브러리 클론 및 파일 이동\n!git clone https://github.com/google/gemma_pytorch.git\n!mkdir -p /kaggle/working/gemma/\n!mv gemma_pytorch/gemma/* /kaggle/working/gemma/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-04T10:30:46.448068Z","iopub.execute_input":"2024-10-04T10:30:46.449076Z","iopub.status.idle":"2024-10-04T10:31:02.936727Z","shell.execute_reply.started":"2024-10-04T10:30:46.449012Z","shell.execute_reply":"2024-10-04T10:31:02.935651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 필수 라이브러리 임포트\nimport sys\nimport os\nimport torch\nimport contextlib\nsys.path.append(\"/kaggle/working/gemma/\") \n\n# Gemma 관련 모듈 임포트\nfrom gemma.config import GemmaConfig, get_model_config\nfrom gemma.model import GemmaForCausalLM\n\n# 모델을 불러오기 위한 경로 설정\nVARIANT = \"2b-v2\"  # 모델 버전\nMACHINE_TYPE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # 디바이스 선택\nweights_dir = '/kaggle/input/gemma-2/pytorch/gemma-2-2b-it/1'  # 모델 체크포인트 경로\nweights_file = os.path.join(weights_dir, \"model.ckpt\")  # 가중치 파일 경로\ntokenizer_path = os.path.join(weights_dir, \"tokenizer.model\")  # 토크나이저 경로\n\n# 텐서 타입 설정을 위한 context manager 정의\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)\n\n# 모델 설정 및 로드\nmodel_config = get_model_config(VARIANT)\nmodel_config.tokenizer = tokenizer_path  # 토크나이저 경로 설정\ndevice = torch.device(MACHINE_TYPE)  # 디바이스 설정\n\nwith _set_default_tensor_type(model_config.get_dtype()):\n    model = GemmaForCausalLM(model_config)  # 모델 초기화\n    model.load_weights(weights_file)  # 가중치 로드\n    model = model.to(device).eval()  # 모델 평가 모드로 전환\n","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:31:02.938605Z","iopub.execute_input":"2024-10-04T10:31:02.938929Z","iopub.status.idle":"2024-10-04T10:31:39.004528Z","shell.execute_reply.started":"2024-10-04T10:31:02.938895Z","shell.execute_reply":"2024-10-04T10:31:39.003311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's inspect the provided Python file to understand its structure and content.\nfile_path = '/kaggle/input/llm-20-questions/keywords.py'\n\n# Read the content of the provided file\nwith open(file_path, 'r') as file:\n    keywords_content = file.read()\n\nkeywords_content","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:31:39.006029Z","iopub.execute_input":"2024-10-04T10:31:39.006623Z","iopub.status.idle":"2024-10-04T10:31:39.021085Z","shell.execute_reply.started":"2024-10-04T10:31:39.006568Z","shell.execute_reply":"2024-10-04T10:31:39.019971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" import textwrap\nfrom IPython.display import display, Markdown\nimport torch\nimport contextlib\nimport os\nfrom gemma.config import get_model_config\nfrom gemma.model import GemmaForCausalLM\n\n# 모델 로드 함수\ndef load_model(VARIANT, device, model_path):\n    WEIGHTS_PATH = model_path\n\n    @contextlib.contextmanager\n    def _set_default_tensor_type(dtype: torch.dtype):\n        \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n        torch.set_default_dtype(dtype)\n        yield\n        torch.set_default_dtype(torch.float)\n\n    # 모델 설정\n    model_config = get_model_config(VARIANT)\n    model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")  # 토크나이저 경로\n    model_config.quant = \"quant\" in VARIANT\n\n    # 모델 로드\n    with _set_default_tensor_type(model_config.get_dtype()):\n        model = GemmaForCausalLM(model_config)\n        ckpt_path = os.path.join(WEIGHTS_PATH, 'model.ckpt')  # 체크포인트 경로\n        model.load_weights(ckpt_path)\n        model = model.to(device).eval()\n\n    return model\n\n# ChatState 클래스 (대화 관리)\nclass ChatState():\n    \"\"\"\n    Manages the conversation history for the \"20 Questions\" game chatbot.\n    \"\"\"\n    __START_TURN_USER__ = \"<start_of_turn>user\\n\"\n    __START_TURN_MODEL__ = \"<start_of_turn>model\\n\"\n    __END_TURN__ = \"<end_of_turn>\\n\"\n\n    def __init__(self, model, device, system=\"\"):\n        \"\"\"\n        Initializes the chat state.\n        Args:\n            model: The language model to use for generating responses.\n            device: The device (e.g., \"cuda\" or \"cpu\") to use for inference.\n            system: (Optional) System instructions or bot description.\n        \"\"\"\n        self.model = model\n        self.device = device\n        self.system = system\n        self.history = []\n        self.question_count = 0  # To track how many questions the bot has asked\n\n    def add_to_history_as_user(self, message):\n        \"\"\"Adds a user message to the history with start/end turn markers.\"\"\"\n        self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n\n    def add_to_history_as_model(self, message):\n        \"\"\"Adds a model response to the history with start/end turn markers.\"\"\"\n        self.history.append(self.__START_TURN_MODEL__ + message + self.__END_TURN__)\n\n    def get_history(self):\n        \"\"\"Returns the entire chat history as a single string.\"\"\"\n        return \"\".join(self.history)\n\n    def get_full_prompt(self):\n        \"\"\"Builds the prompt for the language model, including history and system description.\"\"\"\n        prompt = self.get_history() + self.__START_TURN_MODEL__\n        if len(self.system) > 0:\n            prompt = self.system + \"\\n\" + prompt\n        return prompt\n\n    def send_message(self, message):\n        \"\"\"Handles sending a user message and getting a model response.\"\"\"\n        self.add_to_history_as_user(message)\n        prompt = self.get_full_prompt()\n\n        # Generate the model's response\n        response = self.model.generate(prompt, device=self.device)\n\n        # Handle the response format (if list or string)\n        if isinstance(response, list):\n            result = response[0]\n        else:\n            result = response.replace(prompt, \"\")  # Remove prompt from the response\n\n        self.add_to_history_as_model(result)\n        self.question_count += 1\n\n        return result\n\n# 시스템 프롬프트 설정 (20 Questions 게임을 위한 시스템 설명)\nsystem_prompt = (\n    \"You are an AI assistant playing the 20 Questions game. \"\n    \"In this game, you should ask the most relevant and logical yes-or-no questions. \"\n    \"The goal is to identify a specific object, place, or thing that the user is thinking of. \"\n    \"Ask questions that will help you narrow down the possibilities.\"\n)\n\n# 모델 초기화 (사용 중인 모델 버전에 맞게 수정)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nweights_dir = '/kaggle/input/gemma-2/pytorch/gemma-2-2b-it/1'  # 모델 파일 경로\ngemma_lm = load_model(\"2b-v2\", device, weights_dir)\n\n# 챗봇 상태 초기화 (시스템 설명 포함)\nchat = ChatState(gemma_lm, device, system=system_prompt)\n\n# 대화 출력 함수\ndef display_chat(prompt, text):\n    formatted_prompt = \"🙋‍♂️ \" + prompt + \"\\n\"\n    text = textwrap.indent(text, '> ')\n    formatted_text = \"🤖\\n\" + text + \"\\n\"\n    display(Markdown(formatted_prompt + formatted_text))\n\n# 20 Questions 게임 함수 수정\ndef play_20_questions(chat):\n    print(\"Let's play 20 Questions!\")\n    \n    # 첫 질문을 모델이 생성\n    question = chat.send_message(\"Let's play 20 Questions. I'll start by asking: Is it a living thing?\")\n    display_chat(\"Let's play 20 Questions. I'll start by asking: Is it a living thing?\", question)\n    \n    # 20번의 질문을 진행 (또는 모델이 정답을 맞출 때까지)\n    for _ in range(19):  # 최대 20번 질문\n        answer = input(\"Your answer (yes/no): \").strip().lower()  # 유저의 대답 받기\n        \n        # 'yes' 또는 'no'에 따라 다음 질문을 생성\n        if answer == \"yes\" or answer == \"no\":\n            next_question = chat.send_message(answer)\n            display_chat(answer, next_question)\n        else:\n            print(\"Please answer with 'yes' or 'no'.\")\n            continue\n        \n        # 모델이 구체적인 물체나 장소를 추측하려는 경우 ('Is it a...' 또는 'Is it the...' 형태로 추측)\n        if next_question.lower().startswith(\"is it a\") or next_question.lower().startswith(\"is it the\"):\n            # 모델이 물체나 장소를 추측하려 할 때\n            print(\"Bot is trying to guess...\")\n            print(\"The model's guess: \", next_question)\n            \n            # 정답 확인 (특정 물체나 장소를 추측했을 때만 종료)\n            final_answer = input(\"Is the model's guess correct? (yes/no): \").strip().lower()\n            if final_answer == \"yes\":\n                print(\"The model guessed correctly! The game is over.\")\n                break\n            else:\n                print(\"The model guessed wrong. Continuing the game...\")\n\n        # 모델이 추측을 시도하는데 명확한 답이 아닌 경우에는 계속 진행\n        else:\n            print(\"The model is still narrowing down the options...\")\n    \n    print(\"End of game!\")\n\n# 게임 시작\nplay_20_questions(chat)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:40:16.319405Z","iopub.execute_input":"2024-10-04T10:40:16.319850Z","iopub.status.idle":"2024-10-04T10:41:32.140256Z","shell.execute_reply.started":"2024-10-04T10:40:16.319811Z","shell.execute_reply":"2024-10-04T10:41:32.139243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It's done!!! check it out!**","metadata":{}}]}