{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9431804,"sourceType":"datasetVersion","datasetId":5730280},{"sourceId":85979,"sourceType":"modelInstanceVersion","modelInstanceId":72240,"modelId":76277}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n!pip install -q -U immutabledict sentencepiece\n\n# Gemma PyTorch ë¼ì´ë¸ŒëŸ¬ë¦¬ í´ë¡  ë° íŒŒì¼ ì´ë™\n!git clone https://github.com/google/gemma_pytorch.git\n!mkdir -p /kaggle/working/gemma/\n!mv gemma_pytorch/gemma/* /kaggle/working/gemma/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-04T10:30:46.448068Z","iopub.execute_input":"2024-10-04T10:30:46.449076Z","iopub.status.idle":"2024-10-04T10:31:02.936727Z","shell.execute_reply.started":"2024-10-04T10:30:46.449012Z","shell.execute_reply":"2024-10-04T10:31:02.935651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\nimport sys\nimport os\nimport torch\nimport contextlib\nsys.path.append(\"/kaggle/working/gemma/\") \n\n# Gemma ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸\nfrom gemma.config import GemmaConfig, get_model_config\nfrom gemma.model import GemmaForCausalLM\n\n# ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ ê²½ë¡œ ì„¤ì •\nVARIANT = \"2b-v2\"  # ëª¨ë¸ ë²„ì „\nMACHINE_TYPE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # ë””ë°”ì´ìŠ¤ ì„ íƒ\nweights_dir = '/kaggle/input/gemma-2/pytorch/gemma-2-2b-it/1'  # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ\nweights_file = os.path.join(weights_dir, \"model.ckpt\")  # ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ\ntokenizer_path = os.path.join(weights_dir, \"tokenizer.model\")  # í† í¬ë‚˜ì´ì € ê²½ë¡œ\n\n# í…ì„œ íƒ€ì… ì„¤ì •ì„ ìœ„í•œ context manager ì •ì˜\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)\n\n# ëª¨ë¸ ì„¤ì • ë° ë¡œë“œ\nmodel_config = get_model_config(VARIANT)\nmodel_config.tokenizer = tokenizer_path  # í† í¬ë‚˜ì´ì € ê²½ë¡œ ì„¤ì •\ndevice = torch.device(MACHINE_TYPE)  # ë””ë°”ì´ìŠ¤ ì„¤ì •\n\nwith _set_default_tensor_type(model_config.get_dtype()):\n    model = GemmaForCausalLM(model_config)  # ëª¨ë¸ ì´ˆê¸°í™”\n    model.load_weights(weights_file)  # ê°€ì¤‘ì¹˜ ë¡œë“œ\n    model = model.to(device).eval()  # ëª¨ë¸ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜\n","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:31:02.938605Z","iopub.execute_input":"2024-10-04T10:31:02.938929Z","iopub.status.idle":"2024-10-04T10:31:39.004528Z","shell.execute_reply.started":"2024-10-04T10:31:02.938895Z","shell.execute_reply":"2024-10-04T10:31:39.003311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's inspect the provided Python file to understand its structure and content.\nfile_path = '/kaggle/input/llm-20-questions/keywords.py'\n\n# Read the content of the provided file\nwith open(file_path, 'r') as file:\n    keywords_content = file.read()\n\nkeywords_content","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:31:39.006029Z","iopub.execute_input":"2024-10-04T10:31:39.006623Z","iopub.status.idle":"2024-10-04T10:31:39.021085Z","shell.execute_reply.started":"2024-10-04T10:31:39.006568Z","shell.execute_reply":"2024-10-04T10:31:39.019971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" import textwrap\nfrom IPython.display import display, Markdown\nimport torch\nimport contextlib\nimport os\nfrom gemma.config import get_model_config\nfrom gemma.model import GemmaForCausalLM\n\n# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜\ndef load_model(VARIANT, device, model_path):\n    WEIGHTS_PATH = model_path\n\n    @contextlib.contextmanager\n    def _set_default_tensor_type(dtype: torch.dtype):\n        \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n        torch.set_default_dtype(dtype)\n        yield\n        torch.set_default_dtype(torch.float)\n\n    # ëª¨ë¸ ì„¤ì •\n    model_config = get_model_config(VARIANT)\n    model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")  # í† í¬ë‚˜ì´ì € ê²½ë¡œ\n    model_config.quant = \"quant\" in VARIANT\n\n    # ëª¨ë¸ ë¡œë“œ\n    with _set_default_tensor_type(model_config.get_dtype()):\n        model = GemmaForCausalLM(model_config)\n        ckpt_path = os.path.join(WEIGHTS_PATH, 'model.ckpt')  # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ\n        model.load_weights(ckpt_path)\n        model = model.to(device).eval()\n\n    return model\n\n# ChatState í´ë˜ìŠ¤ (ëŒ€í™” ê´€ë¦¬)\nclass ChatState():\n    \"\"\"\n    Manages the conversation history for the \"20 Questions\" game chatbot.\n    \"\"\"\n    __START_TURN_USER__ = \"<start_of_turn>user\\n\"\n    __START_TURN_MODEL__ = \"<start_of_turn>model\\n\"\n    __END_TURN__ = \"<end_of_turn>\\n\"\n\n    def __init__(self, model, device, system=\"\"):\n        \"\"\"\n        Initializes the chat state.\n        Args:\n            model: The language model to use for generating responses.\n            device: The device (e.g., \"cuda\" or \"cpu\") to use for inference.\n            system: (Optional) System instructions or bot description.\n        \"\"\"\n        self.model = model\n        self.device = device\n        self.system = system\n        self.history = []\n        self.question_count = 0  # To track how many questions the bot has asked\n\n    def add_to_history_as_user(self, message):\n        \"\"\"Adds a user message to the history with start/end turn markers.\"\"\"\n        self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n\n    def add_to_history_as_model(self, message):\n        \"\"\"Adds a model response to the history with start/end turn markers.\"\"\"\n        self.history.append(self.__START_TURN_MODEL__ + message + self.__END_TURN__)\n\n    def get_history(self):\n        \"\"\"Returns the entire chat history as a single string.\"\"\"\n        return \"\".join(self.history)\n\n    def get_full_prompt(self):\n        \"\"\"Builds the prompt for the language model, including history and system description.\"\"\"\n        prompt = self.get_history() + self.__START_TURN_MODEL__\n        if len(self.system) > 0:\n            prompt = self.system + \"\\n\" + prompt\n        return prompt\n\n    def send_message(self, message):\n        \"\"\"Handles sending a user message and getting a model response.\"\"\"\n        self.add_to_history_as_user(message)\n        prompt = self.get_full_prompt()\n\n        # Generate the model's response\n        response = self.model.generate(prompt, device=self.device)\n\n        # Handle the response format (if list or string)\n        if isinstance(response, list):\n            result = response[0]\n        else:\n            result = response.replace(prompt, \"\")  # Remove prompt from the response\n\n        self.add_to_history_as_model(result)\n        self.question_count += 1\n\n        return result\n\n# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì • (20 Questions ê²Œì„ì„ ìœ„í•œ ì‹œìŠ¤í…œ ì„¤ëª…)\nsystem_prompt = (\n    \"You are an AI assistant playing the 20 Questions game. \"\n    \"In this game, you should ask the most relevant and logical yes-or-no questions. \"\n    \"The goal is to identify a specific object, place, or thing that the user is thinking of. \"\n    \"Ask questions that will help you narrow down the possibilities.\"\n)\n\n# ëª¨ë¸ ì´ˆê¸°í™” (ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ë²„ì „ì— ë§ê²Œ ìˆ˜ì •)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nweights_dir = '/kaggle/input/gemma-2/pytorch/gemma-2-2b-it/1'  # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ\ngemma_lm = load_model(\"2b-v2\", device, weights_dir)\n\n# ì±—ë´‡ ìƒíƒœ ì´ˆê¸°í™” (ì‹œìŠ¤í…œ ì„¤ëª… í¬í•¨)\nchat = ChatState(gemma_lm, device, system=system_prompt)\n\n# ëŒ€í™” ì¶œë ¥ í•¨ìˆ˜\ndef display_chat(prompt, text):\n    formatted_prompt = \"ğŸ™‹â€â™‚ï¸ \" + prompt + \"\\n\"\n    text = textwrap.indent(text, '> ')\n    formatted_text = \"ğŸ¤–\\n\" + text + \"\\n\"\n    display(Markdown(formatted_prompt + formatted_text))\n\n# 20 Questions ê²Œì„ í•¨ìˆ˜ ìˆ˜ì •\ndef play_20_questions(chat):\n    print(\"Let's play 20 Questions!\")\n    \n    # ì²« ì§ˆë¬¸ì„ ëª¨ë¸ì´ ìƒì„±\n    question = chat.send_message(\"Let's play 20 Questions. I'll start by asking: Is it a living thing?\")\n    display_chat(\"Let's play 20 Questions. I'll start by asking: Is it a living thing?\", question)\n    \n    # 20ë²ˆì˜ ì§ˆë¬¸ì„ ì§„í–‰ (ë˜ëŠ” ëª¨ë¸ì´ ì •ë‹µì„ ë§ì¶œ ë•Œê¹Œì§€)\n    for _ in range(19):  # ìµœëŒ€ 20ë²ˆ ì§ˆë¬¸\n        answer = input(\"Your answer (yes/no): \").strip().lower()  # ìœ ì €ì˜ ëŒ€ë‹µ ë°›ê¸°\n        \n        # 'yes' ë˜ëŠ” 'no'ì— ë”°ë¼ ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±\n        if answer == \"yes\" or answer == \"no\":\n            next_question = chat.send_message(answer)\n            display_chat(answer, next_question)\n        else:\n            print(\"Please answer with 'yes' or 'no'.\")\n            continue\n        \n        # ëª¨ë¸ì´ êµ¬ì²´ì ì¸ ë¬¼ì²´ë‚˜ ì¥ì†Œë¥¼ ì¶”ì¸¡í•˜ë ¤ëŠ” ê²½ìš° ('Is it a...' ë˜ëŠ” 'Is it the...' í˜•íƒœë¡œ ì¶”ì¸¡)\n        if next_question.lower().startswith(\"is it a\") or next_question.lower().startswith(\"is it the\"):\n            # ëª¨ë¸ì´ ë¬¼ì²´ë‚˜ ì¥ì†Œë¥¼ ì¶”ì¸¡í•˜ë ¤ í•  ë•Œ\n            print(\"Bot is trying to guess...\")\n            print(\"The model's guess: \", next_question)\n            \n            # ì •ë‹µ í™•ì¸ (íŠ¹ì • ë¬¼ì²´ë‚˜ ì¥ì†Œë¥¼ ì¶”ì¸¡í–ˆì„ ë•Œë§Œ ì¢…ë£Œ)\n            final_answer = input(\"Is the model's guess correct? (yes/no): \").strip().lower()\n            if final_answer == \"yes\":\n                print(\"The model guessed correctly! The game is over.\")\n                break\n            else:\n                print(\"The model guessed wrong. Continuing the game...\")\n\n        # ëª¨ë¸ì´ ì¶”ì¸¡ì„ ì‹œë„í•˜ëŠ”ë° ëª…í™•í•œ ë‹µì´ ì•„ë‹Œ ê²½ìš°ì—ëŠ” ê³„ì† ì§„í–‰\n        else:\n            print(\"The model is still narrowing down the options...\")\n    \n    print(\"End of game!\")\n\n# ê²Œì„ ì‹œì‘\nplay_20_questions(chat)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:40:16.319405Z","iopub.execute_input":"2024-10-04T10:40:16.319850Z","iopub.status.idle":"2024-10-04T10:41:32.140256Z","shell.execute_reply.started":"2024-10-04T10:40:16.319811Z","shell.execute_reply":"2024-10-04T10:41:32.139243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It's done!!! check it out!**","metadata":{}}]}